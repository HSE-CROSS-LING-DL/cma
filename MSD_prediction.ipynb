{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_project_MorphAnalysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVLciLNFwPM6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://raw.githubusercontent.com/HSE-CROSS-LING-DL/cma/master/data/trk-uncovered-train-transliterated.csv -O train.csv\n",
        "!wget https://raw.githubusercontent.com/HSE-CROSS-LING-DL/cma/master/data/trk-uncovered-dev-transliterated.csv -O dev.csv\n",
        "!wget https://raw.githubusercontent.com/HSE-CROSS-LING-DL/cma/master/data/trk-uncovered-test-transliterated.csv -O test.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cbfFqc8wqIN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sk9ByVKVxDA6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv('train.csv')\n",
        "dev = pd.read_csv('dev.csv')\n",
        "test = pd.read_csv('test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ld2jRMn521iy",
        "colab_type": "code",
        "outputId": "a6886662-5d2a-4728-8288-b1549fb0c2ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train = train.dropna()\n",
        "train = train.reset_index(drop=True)\n",
        "train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(79228, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spjvizmo8tLE",
        "colab_type": "code",
        "outputId": "eb4a47a2-2bb4-454a-f03a-ad0327bbd8c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dev = dev.dropna()\n",
        "dev = dev.reset_index(drop=True)\n",
        "dev.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1245, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RG24GN92xGOT",
        "colab_type": "code",
        "outputId": "6b0f2f91-60cc-4a3f-99ae-1dcc1c3a4d54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test = test.dropna()\n",
        "test = test.reset_index(drop=True)\n",
        "test.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12926, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNimA6vSFTPu",
        "colab_type": "code",
        "outputId": "3b6e11e5-d81e-4d54-837d-f5eb17ffb086",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "train.head(1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lang</th>\n",
              "      <th>word</th>\n",
              "      <th>lemma</th>\n",
              "      <th>pos</th>\n",
              "      <th>morph</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tat</td>\n",
              "      <td>казак</td>\n",
              "      <td>казак</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>Case=Nom</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  lang   word  lemma   pos     morph\n",
              "0  tat  казак  казак  NOUN  Case=Nom"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FGs0JC39MyM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def agg_tags(series):\n",
        "    return sorted(set(series.str.cat(sep='|').split('|')))\n",
        "\n",
        "from sklearn.preprocessing import MultiLabelBinarizer, LabelBinarizer\n",
        "\n",
        "def preprocess_dataset(dataset_df):\n",
        "\n",
        "    dataset_agg = dataset_df.groupby([\"word\", \"pos\"]).agg({\"morph\" : agg_tags, \n",
        "                                                    \"word\": \"first\", \"pos\": \"first\"})\n",
        "\n",
        "    mlb = MultiLabelBinarizer()\n",
        "    dataset_df_morph_mhe = pd.DataFrame(mlb.fit_transform(dataset_agg.morph),\n",
        "                    columns=map(lambda x: \"morph_mhe_\"+x, mlb.classes_),\n",
        "                    index=dataset_agg.index)\n",
        "\n",
        "    oher = LabelBinarizer()\n",
        "\n",
        "    dataset_df_pos_mhe = pd.DataFrame(oher.fit_transform(dataset_agg.pos),\n",
        "                    columns=map(lambda x: \"pos_mhe_\"+x, oher.classes_),\n",
        "                    index=dataset_agg.index)\n",
        "\n",
        "\n",
        "    dataset_new = dataset_agg.join(dataset_df_morph_mhe).join(dataset_df_pos_mhe)\n",
        "    \n",
        "    return dataset_new\n",
        "\n",
        "train = preprocess_dataset(train)\n",
        "dev = preprocess_dataset(dev)\n",
        "test = preprocess_dataset(test)\n",
        "test[\"morph_mhe_Aspect=Imp\"] = test[\"morph_mhe_Aspect=Impf\"]\n",
        "test.drop(\"morph_mhe_Aspect=Impf\", 1, inplace=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9PRE0ZXs-4L",
        "colab_type": "code",
        "outputId": "ca99bc73-e936-4e68-c81a-b8e28b58eae0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(49110, 64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bMlaNmr_fAo",
        "colab_type": "code",
        "outputId": "420561a6-fdf0-407b-8b63-c012d0eda433",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "test.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>morph</th>\n",
              "      <th>word</th>\n",
              "      <th>pos</th>\n",
              "      <th>morph_mhe_Aspect=Perf</th>\n",
              "      <th>morph_mhe_Case=Abl</th>\n",
              "      <th>morph_mhe_Case=Acc</th>\n",
              "      <th>morph_mhe_Case=Dat</th>\n",
              "      <th>morph_mhe_Case=Gen</th>\n",
              "      <th>morph_mhe_Case=Loc</th>\n",
              "      <th>morph_mhe_Case=Nom</th>\n",
              "      <th>morph_mhe_Degree=Comp</th>\n",
              "      <th>morph_mhe_Deriv=Coop</th>\n",
              "      <th>morph_mhe_Mood=Imp</th>\n",
              "      <th>morph_mhe_Mood=Opt</th>\n",
              "      <th>morph_mhe_Number=Plur</th>\n",
              "      <th>morph_mhe_Number=Sing</th>\n",
              "      <th>morph_mhe_Number[psor]=Plur</th>\n",
              "      <th>morph_mhe_Number[psor]=Sing</th>\n",
              "      <th>morph_mhe_Number[psor]=Sing,Plur</th>\n",
              "      <th>morph_mhe_Person=1</th>\n",
              "      <th>morph_mhe_Person=2</th>\n",
              "      <th>morph_mhe_Person=3</th>\n",
              "      <th>morph_mhe_Person[psor]=1</th>\n",
              "      <th>morph_mhe_Person[psor]=2</th>\n",
              "      <th>morph_mhe_Person[psor]=3</th>\n",
              "      <th>morph_mhe_Polarity=Neg</th>\n",
              "      <th>morph_mhe_Tense=Aor</th>\n",
              "      <th>morph_mhe_Tense=Fut</th>\n",
              "      <th>morph_mhe_Tense=Past</th>\n",
              "      <th>morph_mhe_Tense=Pres</th>\n",
              "      <th>morph_mhe_Valency=1</th>\n",
              "      <th>morph_mhe_Valency=2</th>\n",
              "      <th>morph_mhe_VerbForm=Conv</th>\n",
              "      <th>morph_mhe_VerbForm=Fin</th>\n",
              "      <th>morph_mhe_VerbForm=Part</th>\n",
              "      <th>morph_mhe_VerbForm=Vnoun</th>\n",
              "      <th>morph_mhe_Voice=Pass</th>\n",
              "      <th>morph_mhe__</th>\n",
              "      <th>pos_mhe_ADJ</th>\n",
              "      <th>pos_mhe_ADV</th>\n",
              "      <th>pos_mhe_NOUN</th>\n",
              "      <th>pos_mhe_VERB</th>\n",
              "      <th>morph_mhe_Aspect=Imp</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>word</th>\n",
              "      <th>pos</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>абаза</th>\n",
              "      <th>NOUN</th>\n",
              "      <td>[Case=Nom]</td>\n",
              "      <td>абаза</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>абазалыла</th>\n",
              "      <th>NOUN</th>\n",
              "      <td>[Case=Nom, Number=Plur]</td>\n",
              "      <td>абазалыла</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>абазалыланы</th>\n",
              "      <th>NOUN</th>\n",
              "      <td>[Case=Acc, Case=Gen, Number=Plur]</td>\n",
              "      <td>абазалыланы</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>аббревиатура</th>\n",
              "      <th>NOUN</th>\n",
              "      <td>[Case=Nom]</td>\n",
              "      <td>аббревиатура</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>абзац</th>\n",
              "      <th>NOUN</th>\n",
              "      <td>[Case=Nom]</td>\n",
              "      <td>абзац</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               morph  ... morph_mhe_Aspect=Imp\n",
              "word         pos                                      ...                     \n",
              "абаза        NOUN                         [Case=Nom]  ...                    0\n",
              "абазалыла    NOUN            [Case=Nom, Number=Plur]  ...                    0\n",
              "абазалыланы  NOUN  [Case=Acc, Case=Gen, Number=Plur]  ...                    0\n",
              "аббревиатура NOUN                         [Case=Nom]  ...                    0\n",
              "абзац        NOUN                         [Case=Nom]  ...                    0\n",
              "\n",
              "[5 rows x 43 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfDzMkKBrR_L",
        "colab_type": "text"
      },
      "source": [
        "Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcIcfh7krTpr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IT9VyHAMs6xd",
        "colab_type": "code",
        "outputId": "ccce4531-524f-4d6a-caeb-c6f71f18079c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "char2index = {'PAD': 0}\n",
        "\n",
        "for word in tqdm(train.word):\n",
        "  for char in word:\n",
        "    if char not in char2index:\n",
        "      char2index[char] = len(char2index)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 49110/49110 [00:00<00:00, 600216.42it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iakvAEJFrdmf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# unique_morphotags = sorted({morphotag \n",
        "#                      for morphoanalysis in pd.concat([test.morph, train.morph]) \n",
        "#                      for morphotag in morphoanalysis.split('|')})\n",
        "\n",
        "# idx2morphtag = {idx: morphtag for idx, morphtag in enumerate(unique_morphotags)}\n",
        "# morphtag2idx = {morphtag: idx for idx, morphtag in enumerate(unique_morphotags)}\n",
        "\n",
        "\n",
        "mhe_morph_colnames = [c for c in train.columns if c.startswith(\"morph_mhe_\")]\n",
        "mhe_pos_colnames = [c for c in train.columns if c.startswith(\"pos_mhe_\")]\n",
        "\n",
        "for col in set(mhe_morph_colnames).difference(set(test.columns)):\n",
        "    test[col] = test.apply(lambda x: 0, axis=1)\n",
        "for col in set(mhe_morph_colnames).difference(set(dev.columns)):\n",
        "    dev[col] = dev.apply(lambda x: 0, axis=1)\n",
        "\n",
        "\n",
        "class VectorizedData(Dataset):\n",
        "    \n",
        "    def __init__(self, df, char2index=char2index, sequence_length=12, pad_token = 'PAD', verbose=True):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.x_data = []\n",
        "        self.y_data = []\n",
        "\n",
        "        self.char2index = char2index\n",
        "\n",
        "        self.sequence_length = sequence_length\n",
        "        self.pad_token = pad_token\n",
        "        self.pad_index = self.char2index[self.pad_token]\n",
        "        # self.fasttext_model = fasttext_model\n",
        "\n",
        "        self.load(df, verbose=verbose)\n",
        "\n",
        "    # def vectorize(self, word, fasttext_model=ft_model):\n",
        "    #   try:\n",
        "    #     return self.fasttext_model[word]\n",
        "    #   except KeyError:\n",
        "    #     pass\n",
        "\n",
        "    def preprocess(self, word):\n",
        "      return [c for c in word]\n",
        "\n",
        "    def indexing(self, chars):\n",
        "      return [self.char2index[char] for char in chars if char in self.char2index]\n",
        "\n",
        "    def load(self, data, verbose=True):\n",
        "        for index, row in tqdm(data.iterrows(), desc='Loading data', disable=not verbose):\n",
        "            # ft_x_vector = self.vectorize(row[\"word\"], fasttext_model)\n",
        "            # ft_x_vector = np.concatenate([ft_x_vector, row[mhe_pos_colnames].astype('float')])\n",
        "            chars = self.preprocess(row[\"word\"])\n",
        "            indexed_chars = self.indexing(chars)\n",
        "\n",
        "            y_vector = np.concatenate([[], row[mhe_morph_colnames].astype('float')])\n",
        "            self.x_data.append(indexed_chars)\n",
        "            self.y_data.append(y_vector)\n",
        "\n",
        "    def padding(self, sequence):\n",
        "        if len(sequence) > self.sequence_length:\n",
        "          sequence = sequence[:self.sequence_length]\n",
        "        elif len(sequence) < self.sequence_length:\n",
        "          sequence = sequence + [self.pad_index] * (self.sequence_length - len(sequence))\n",
        "        return sequence\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x_data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        x = self.x_data[idx]\n",
        "        x = self.padding(x)\n",
        "        x = torch.Tensor(x).long()\n",
        "        y = self.y_data[idx]\n",
        "        \n",
        "        return x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8j9azXHnuxvC",
        "colab_type": "code",
        "outputId": "44a3af44-d12a-4144-fb35-674c1f4eec99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_dataset = VectorizedData(train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data: 49110it [00:43, 1130.01it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGDwm6_F_oWN",
        "colab_type": "code",
        "outputId": "c8e9f9a1-462b-457c-d16a-e7af235c1b55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_dataset = VectorizedData(test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data: 8966it [00:07, 1164.15it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBwdHeysxxfE",
        "colab_type": "code",
        "outputId": "d4995883-31bb-4d1c-c165-68755f848078",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dev_dataset = VectorizedData(dev)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data: 1025it [00:01, 1021.30it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqDyI6ysxKxI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_loader = DataLoader(train_dataset, batch_size=64)\n",
        "test_data_loader = DataLoader(test_dataset, batch_size=64)\n",
        "dev_data_loader = DataLoader(dev_dataset, batch_size=64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQjHAZ3MzBi1",
        "colab_type": "text"
      },
      "source": [
        "Модель"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UArPpzKJz7eh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import nn\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
        "from sklearn.utils.class_weight import compute_class_weight"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmsWKJiRzDR8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MorphoTagger(nn.Module):\n",
        "\n",
        "  def __init__(self, embedding_dim=104, n_classes=len(mhe_morph_colnames), \\\n",
        "               vocab_size=len(char2index), hidden_dim=64, seq_len=12, \\\n",
        "               ngrams=[2,3,4], keep_proba=0.4):\n",
        "    super().__init__()\n",
        "\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.hidden_dim = hidden_dim\n",
        "\n",
        "    self.dropout = nn.Dropout(p=keep_proba)\n",
        "\n",
        "    self.embedding_layer = nn.Embedding(num_embeddings=self.vocab_size, \\\n",
        "                                        embedding_dim=self.embedding_dim)\n",
        "    \n",
        "    self.lstm_layer = nn.LSTM(self.embedding_dim, self.hidden_dim, batch_first=True, bidirectional=True)\n",
        "\n",
        "    self.convs = nn.ModuleList([nn.Conv1d(in_channels=self.hidden_dim*2, \n",
        "                                          out_channels=self.hidden_dim, \n",
        "                                          kernel_size=n) for n in ngrams])\n",
        "    \n",
        "    self.pooling = nn.ModuleList([nn.MaxPool1d(kernel_size=seq_len-n+1) for n in ngrams])\n",
        "\n",
        "    # self.linear1 = nn.Linear(in_features=embedding_dim, out_features=64)\n",
        "    self.linear = nn.Linear(in_features=len(ngrams) * self.hidden_dim, out_features=n_classes)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.embedding_layer(x)\n",
        "    lstm_x, mem = self.lstm_layer(x)\n",
        "\n",
        "    x_transposed = lstm_x.transpose(1, 2)\n",
        "\n",
        "    conved = [conv(x_transposed) for conv in self.convs]\n",
        "    pooled = [pool(conv).squeeze(-1) for pool, conv in zip(self.pooling, conved)]\n",
        "    cat = self.dropout(torch.cat(pooled, 1))\n",
        "    out = self.linear(cat)\n",
        "\n",
        "    return self.sigmoid(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlbUKkWFEwWe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = MorphoTagger()\n",
        "\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(params=model.parameters())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XH15CwcFAKe",
        "colab_type": "code",
        "outputId": "55e968bb-9b01-4d78-ae7b-aeca0f9a6e93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "from sklearn.metrics import label_ranking_loss\n",
        "\n",
        "\n",
        "epochs = 30\n",
        "losses = []\n",
        "best_test_loss = 10.\n",
        "\n",
        "acc, macros, prec, rec = [], [], [], []\n",
        "label_ranking_losses = []\n",
        "for n_epoch in range(epochs):\n",
        "    \n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    test_preds = []\n",
        "    test_targets = []\n",
        "    test_pred_class = []\n",
        "    \n",
        "    progress_bar = tqdm(total=len(train_data_loader.dataset), desc='Epoch {}'.format(n_epoch + 1))\n",
        "    \n",
        "    tmp_idx = 0\n",
        "    for x, y in train_data_loader:\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        pred = model(x)\n",
        "        y = y.float()\n",
        "        \n",
        "        loss = criterion(pred, y)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        train_losses.append(loss.item())\n",
        "        losses.append(loss.item())\n",
        "        \n",
        "        progress_bar.set_postfix(train_loss = np.mean(losses[-500:]))\n",
        "\n",
        "        progress_bar.update(x.shape[0])\n",
        "        \n",
        "    progress_bar.close()\n",
        "    \n",
        "    for x, y in test_data_loader:\n",
        "      with torch.no_grad():\n",
        "            \n",
        "        pred = model(x) # .float())\n",
        "\n",
        "        test_preds.append(pred.numpy())\n",
        "        test_targets.append(y.float().numpy())\n",
        "\n",
        "\n",
        "        test_pred_class.append(np.argmax(pred, axis=1))\n",
        "\n",
        "        loss = criterion(pred, y.float())\n",
        "\n",
        "        test_losses.append(loss.item())\n",
        "        \n",
        "    mean_test_loss = np.mean(test_losses)\n",
        "\n",
        "    test_targets = np.concatenate(test_targets).squeeze()\n",
        "    test_pred_class = np.concatenate(test_preds).squeeze()\n",
        "    \n",
        "    # accuracy = accuracy_score(test_targets, test_pred_class)\n",
        "    # precision = precision_score(test_targets, test_pred_class, average='macro')\n",
        "    # recall = recall_score(test_targets, test_pred_class, average='macro')\n",
        "    # f1 = f1_score(test_targets, test_pred_class, average='macro')\n",
        "\n",
        "    # acc.append(accuracy)\n",
        "    # macros.append(f1)\n",
        "    # prec.append(precision)\n",
        "    # rec.append(recall)\n",
        "    label_ranking_losses.append(label_ranking_loss(test_targets, test_pred_class))\n",
        "\n",
        "    print(label_ranking_losses[-1])\n",
        "\n",
        "    if mean_test_loss < best_test_loss:\n",
        "        best_test_loss = mean_test_loss\n",
        "    else:\n",
        "        print('Early stopping')\n",
        "        break\n",
        "    # print('Losses: train - {:.3f}, test - {:.3f}'.format(np.mean(train_losses), mean_test_loss))\n",
        "\n",
        "    # print('Test: accuracy - {:.3f}, precision - {:.3f}, recall - {:.3f}, f1 macro - {:.3f}'.format(accuracy, precision, recall, f1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 49110/49110 [00:26<00:00, 1847.89it/s, train_loss=0.094]\n",
            "Epoch 2:   1%|          | 256/49110 [00:00<00:26, 1844.09it/s, train_loss=0.0939]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.07357421026940943\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|██████████| 49110/49110 [00:26<00:00, 1862.80it/s, train_loss=0.0723]\n",
            "Epoch 3:   1%|          | 256/49110 [00:00<00:26, 1851.69it/s, train_loss=0.0726]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.07264052021242942\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|██████████| 49110/49110 [00:26<00:00, 1874.69it/s, train_loss=0.0646]\n",
            "Epoch 4:   1%|          | 256/49110 [00:00<00:28, 1697.24it/s, train_loss=0.0648]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.07072471846551878\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|██████████| 49110/49110 [00:26<00:00, 1853.96it/s, train_loss=0.06]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.07245349041587791\n",
            "Early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sokJt_Fc__NF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions, targets = [], []\n",
        "for x_batch, y_batch in test_data_loader:\n",
        "    with torch.no_grad():\n",
        "        pred_batch = model(x_batch)\n",
        "    predictions.extend(pred_batch)\n",
        "    targets.extend(y_batch.float())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3x0liFPsbCgN",
        "colab_type": "code",
        "outputId": "cef8fa1f-86fd-496f-8d1b-d4a512dc0db6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# !cp /content/drive/My\\ Drive/thresholds.npy .\n",
        "class_thersholds = [0.2]*57\n",
        "\n",
        "def score_model(preds, targets, thresholds):\n",
        "    precisions, recalls, fscores = [], [], []\n",
        "    precisions_sum, recalls_sum, fscores_sum = 0, 0, 0\n",
        "    for idx, (pred, target) in enumerate(zip(preds, targets)):\n",
        "        if idx % 100 == 0:\n",
        "            print(idx, len(targets))\n",
        "        tp, fp, tn, fn = [], [], [], []\n",
        "        tp, fp, tn, fn = [], [], [], []\n",
        "        for class_idx in range(len(pred)):\n",
        "            class_is_predicted = pred[class_idx] > thresholds[class_idx]\n",
        "            class_is_required = bool(target[class_idx])\n",
        "            if class_is_predicted:\n",
        "                if class_is_required:\n",
        "                    tp.append(class_idx)\n",
        "                else:\n",
        "                    fp.append(class_idx)\n",
        "            else:\n",
        "                if not class_is_required:\n",
        "                    tn.append(class_idx)\n",
        "                else:\n",
        "                    fn.append(class_idx)\n",
        "        precision = len(tp) / (len(tp) + len(fp)) if any((tp, fp)) else 0\n",
        "        recall = len(tp) / (len(tp) + len(fn)) if any((tp, fn)) else 0\n",
        "        fscore = 2 * precision* recall / (precision + recall) if any((precision, recall)) else 0\n",
        "\n",
        "        precisions.append(precision)\n",
        "        precisions_sum += precision\n",
        "        recalls.append(recall)\n",
        "        recalls_sum += recall\n",
        "        fscores.append(fscores)\n",
        "        fscores_sum += fscore\n",
        "\n",
        "\n",
        "    return precisions_sum/(idx+1), recalls_sum/(idx+1), fscores_sum/(idx+1)\n",
        "\n",
        "msd_res = score_model(predictions, targets, class_thersholds)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 8966\n",
            "100 8966\n",
            "200 8966\n",
            "300 8966\n",
            "400 8966\n",
            "500 8966\n",
            "600 8966\n",
            "700 8966\n",
            "800 8966\n",
            "900 8966\n",
            "1000 8966\n",
            "1100 8966\n",
            "1200 8966\n",
            "1300 8966\n",
            "1400 8966\n",
            "1500 8966\n",
            "1600 8966\n",
            "1700 8966\n",
            "1800 8966\n",
            "1900 8966\n",
            "2000 8966\n",
            "2100 8966\n",
            "2200 8966\n",
            "2300 8966\n",
            "2400 8966\n",
            "2500 8966\n",
            "2600 8966\n",
            "2700 8966\n",
            "2800 8966\n",
            "2900 8966\n",
            "3000 8966\n",
            "3100 8966\n",
            "3200 8966\n",
            "3300 8966\n",
            "3400 8966\n",
            "3500 8966\n",
            "3600 8966\n",
            "3700 8966\n",
            "3800 8966\n",
            "3900 8966\n",
            "4000 8966\n",
            "4100 8966\n",
            "4200 8966\n",
            "4300 8966\n",
            "4400 8966\n",
            "4500 8966\n",
            "4600 8966\n",
            "4700 8966\n",
            "4800 8966\n",
            "4900 8966\n",
            "5000 8966\n",
            "5100 8966\n",
            "5200 8966\n",
            "5300 8966\n",
            "5400 8966\n",
            "5500 8966\n",
            "5600 8966\n",
            "5700 8966\n",
            "5800 8966\n",
            "5900 8966\n",
            "6000 8966\n",
            "6100 8966\n",
            "6200 8966\n",
            "6300 8966\n",
            "6400 8966\n",
            "6500 8966\n",
            "6600 8966\n",
            "6700 8966\n",
            "6800 8966\n",
            "6900 8966\n",
            "7000 8966\n",
            "7100 8966\n",
            "7200 8966\n",
            "7300 8966\n",
            "7400 8966\n",
            "7500 8966\n",
            "7600 8966\n",
            "7700 8966\n",
            "7800 8966\n",
            "7900 8966\n",
            "8000 8966\n",
            "8100 8966\n",
            "8200 8966\n",
            "8300 8966\n",
            "8400 8966\n",
            "8500 8966\n",
            "8600 8966\n",
            "8700 8966\n",
            "8800 8966\n",
            "8900 8966\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEH0NnzlXE-M",
        "colab_type": "code",
        "outputId": "b5f9d78f-1118-4e98-94b1-6f2e01532808",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "msd_res"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.5155925740864236, 0.6507549242464463, 0.5369942173729173)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-9ChWLhhCRl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save(\"/content/drive/My Drive/msd_res.npy\", msd_res)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxvyyto_f3K9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "msd_res"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}